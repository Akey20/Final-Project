{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (3.6.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from nltk) (2022.4.24)\n",
      "Requirement already satisfied: joblib in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from click->nltk) (2.0.0)\n",
      "Requirement already satisfied: importlib-resources; python_version < \"3.7\" in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from tqdm->nltk) (5.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nsososa1\\anaconda3\\envs\\pythondata\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk) (3.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nsososa1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nsososa1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Natural Language Toolkit (NLTK) = Python library for working with human language data\n",
    "# https://www.nltk.org/\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing .csv to dataframes\n",
    "df_fake = pd.read_csv(\"Resources/Fake1.csv\")\n",
    "df_true = pd.read_csv(\"Resources/True.csv\")\n",
    "\n",
    "# Adding labels to each article. \"1\" for fake, \"0\" for true\n",
    "df_fake['label'] = '1'\n",
    "df_true['label'] = '0'\n",
    "\n",
    "# Combining df_fake and df_true\n",
    "frames = [df_fake, df_true]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "# Changing 'text' column to string type\n",
    "df['text'] = df['text'].astype('string')\n",
    "\n",
    "# Removing null values in 'text' column with '' if present\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "df = df.reset_index(drop = True)\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "token = df['text'].apply(word_tokenize)\n",
    "\n",
    "# Removing stop words\n",
    "# How to remove stop words: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = []\n",
    "\n",
    "for j in range(len(token)):\n",
    "    holder = []\n",
    "    for i in token[j]:\n",
    "        if i not in stop_words:\n",
    "            holder.append(i)\n",
    "    filtered.append(holder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Donald Trump wish Americans Happy New Year lea...\n",
       "1        House Intelligence Committee Chairman Devin Nu...\n",
       "2        On Friday , revealed former Milwaukee Sheriff ...\n",
       "3        On Christmas day , Donald Trump announced woul...\n",
       "4        Pope Francis used annual Christmas Day message...\n",
       "                               ...                        \n",
       "44893    BRUSSELS ( Reuters ) - NATO allies Tuesday wel...\n",
       "44894    LONDON ( Reuters ) - LexisNexis , provider leg...\n",
       "44895    MINSK ( Reuters ) - In shadow disused Soviet-e...\n",
       "44896    MOSCOW ( Reuters ) - Vatican Secretary State C...\n",
       "44897    JAKARTA ( Reuters ) - Indonesia buy 11 Sukhoi ...\n",
       "Name: text, Length: 44898, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rejoining the text into one string. During tokenization, the text was split into separate strings.\n",
    "## i.e, each word in the article's text became its own string\n",
    "for i in range(len(filtered)):\n",
    "    filtered[i] = ' '.join(filtered[i])\n",
    "df['text'] = filtered\n",
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Separting dataframe into features and targets\n",
    "feature = df['text']\n",
    "target = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 119897)\t0.0497509976449245\n",
      "  (0, 119885)\t0.2217222885947209\n",
      "  (0, 119827)\t0.0727218151905891\n",
      "  (0, 118907)\t0.027099627386701335\n",
      "  (0, 118725)\t0.02297364239648796\n",
      "  (0, 118605)\t0.01953806727129072\n",
      "  (0, 118585)\t0.029775975460860492\n",
      "  (0, 118493)\t0.03349758402947387\n",
      "  (0, 118166)\t0.17306222270488847\n",
      "  (0, 117645)\t0.029380489553013995\n",
      "  (0, 117590)\t0.03202705513939177\n",
      "  (0, 117399)\t0.022347402321379373\n",
      "  (0, 117350)\t0.02263150128556617\n",
      "  (0, 117120)\t0.022479015047398857\n",
      "  (0, 117109)\t0.07549997471377655\n",
      "  (0, 117102)\t0.052082448164983045\n",
      "  (0, 117063)\t0.07549997471377655\n",
      "  (0, 116265)\t0.03795849321107084\n",
      "  (0, 115637)\t0.027864652708639755\n",
      "  (0, 113743)\t0.07267882372603385\n",
      "  (0, 111273)\t0.01929270373557012\n",
      "  (0, 111210)\t0.03239784629433769\n",
      "  (0, 111200)\t0.02887622274967753\n",
      "  (0, 111195)\t0.056707091976635364\n",
      "  (0, 110415)\t0.08193625655842116\n",
      "  :\t:\n",
      "  (44897, 27887)\t0.09612304634420085\n",
      "  (44897, 27357)\t0.04849732197614404\n",
      "  (44897, 27273)\t0.2883691390326026\n",
      "  (44897, 26700)\t0.08426068822125461\n",
      "  (44897, 23254)\t0.12581771511822407\n",
      "  (44897, 22027)\t0.05813720658386087\n",
      "  (44897, 21789)\t0.05814711588616716\n",
      "  (44897, 18117)\t0.04716686904973524\n",
      "  (44897, 18054)\t0.06979617892975915\n",
      "  (44897, 14764)\t0.16674798845720226\n",
      "  (44897, 13802)\t0.06701409852452808\n",
      "  (44897, 13800)\t0.06084082014818339\n",
      "  (44897, 11758)\t0.04095915419829291\n",
      "  (44897, 11684)\t0.057930866891990104\n",
      "  (44897, 11272)\t0.024005813809767792\n",
      "  (44897, 10306)\t0.04860022040299808\n",
      "  (44897, 9971)\t0.1075442911029798\n",
      "  (44897, 9313)\t0.05838727414749071\n",
      "  (44897, 8923)\t0.09319449911054821\n",
      "  (44897, 6269)\t0.10577899990482237\n",
      "  (44897, 5694)\t0.12676345928652621\n",
      "  (44897, 5382)\t0.05969435325608139\n",
      "  (44897, 2019)\t0.03745734721920497\n",
      "  (44897, 968)\t0.053725042561786264\n",
      "  (44897, 716)\t0.047812579576775885\n"
     ]
    }
   ],
   "source": [
    "## Vectorization\n",
    "cv = CountVectorizer()\n",
    "cv.fit_transform(feature)\n",
    "ftm = cv.transform(feature)\n",
    "\n",
    "# tfidf = term frequency - inverse document frequency\n",
    "tfidf = TfidfTransformer(norm = 'l2')\n",
    "tfidf.fit(ftm)\n",
    "tf_df_matrix = tfidf.fit_transform(ftm)\n",
    "\n",
    "print(tf_df_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.42093541202672\n"
     ]
    }
   ],
   "source": [
    "## Modeling\n",
    "# Splitting the data into test data and train data\n",
    "x_train, x_test, y_train, y_test = train_test_split(tf_df_matrix, target, random_state = 0)\n",
    "\n",
    "# Training the model\n",
    "# history = model.fit(x_train, y_train, batch_size = 128, epochs = 10, verbose = 1, validation_split = 0.2)\n",
    "\n",
    "# # Naives-Bayes classification method\n",
    "# # Naives-Bayes calculates the possibility of whether a data point belongs within\n",
    "# # a certain category or does not. https://monkeylearn.com/blog/classification-algorithms/\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# NB = MultinomialNB()\n",
    "# NB.fit(x_train, y_train)\n",
    "# accuracy_nb  = NB.score(x_test, y_test)\n",
    "# print(accuracy_nb*100)\n",
    "\n",
    "# Passive Agressive Classifier classification method\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "pac = PassiveAggressiveClassifier(max_iter = 50)\n",
    "pac.fit(x_train, y_train)\n",
    "y_pred = pac.predict(x_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {score*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-PythonData] *",
   "language": "python",
   "name": "conda-env-.conda-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
